#!/usr/bin/env python3
"""
Instance-level Dataset Loader for Tree Species Classification

Loads pre-computed SAM2 instances from manifest files.
Each instance is a single tree crown mask with a species label.

Simple. Direct. No complexity.
"""
from __future__ import annotations

import json
import random
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
import torchvision.transforms as transforms
import torchvision.transforms.functional as TF
from PIL import Image
from torch.utils.data import Dataset
from torchvision.transforms import InterpolationMode


class InstanceTransform:
    """Apply synchronized transforms to an image/mask pair."""

    def __init__(
        self,
        image_size: Tuple[int, int] = (224, 224),
        augment: bool = True,
        rotation_degrees: float = 15.0,
        color_jitter_params: Optional[Dict[str, float]] = None,
    ):
        self.image_size = tuple(image_size)
        self.augment = augment
        self.rotation_degrees = rotation_degrees if augment else 0.0
        self.normalize = transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225],
        )
        if augment:
            params = color_jitter_params or {
                'brightness': 0.2,
                'contrast': 0.2,
                'saturation': 0.2,
            }
            self.color_jitter = transforms.ColorJitter(**params)
        else:
            self.color_jitter = None

    def _resize(self, image: Image.Image, mask: Image.Image) -> Tuple[Image.Image, Image.Image]:
        image = TF.resize(image, self.image_size, interpolation=InterpolationMode.BILINEAR)
        mask = TF.resize(mask, self.image_size, interpolation=InterpolationMode.NEAREST)
        return image, mask

    def _random_horizontal_flip(self, image: Image.Image, mask: Image.Image) -> Tuple[Image.Image, Image.Image]:
        if random.random() < 0.5:
            image = TF.hflip(image)
            mask = TF.hflip(mask)
        return image, mask

    def _random_vertical_flip(self, image: Image.Image, mask: Image.Image) -> Tuple[Image.Image, Image.Image]:
        if random.random() < 0.5:
            image = TF.vflip(image)
            mask = TF.vflip(mask)
        return image, mask

    def _random_rotation(self, image: Image.Image, mask: Image.Image) -> Tuple[Image.Image, Image.Image]:
        if self.rotation_degrees > 0:
            angle = random.uniform(-self.rotation_degrees, self.rotation_degrees)
            image = TF.rotate(
                image,
                angle,
                interpolation=InterpolationMode.BILINEAR,
                fill=0,
            )
            mask = TF.rotate(
                mask,
                angle,
                interpolation=InterpolationMode.NEAREST,
                fill=0,
            )
        return image, mask

    def __call__(self, image: Image.Image, mask: Image.Image) -> Tuple[torch.Tensor, torch.Tensor]:
        image, mask = self._resize(image, mask)

        if self.augment:
            image, mask = self._random_horizontal_flip(image, mask)
            image, mask = self._random_vertical_flip(image, mask)
            image, mask = self._random_rotation(image, mask)
            image = self.color_jitter(image)

        image_tensor = TF.to_tensor(image)
        mask_tensor = TF.to_tensor(mask)
        image_tensor = self.normalize(image_tensor)

        return image_tensor, mask_tensor


class TreeInstanceDataset(Dataset):
    """
    Dataset for tree instance classification.
    
    Loads instances from JSONL manifest files generated by precompute_sam2_instances.py.
    Each sample is a single tree crown (mask + image region + species label).
    
    Parameters
    ----------
    manifest_path : Path
        Path to instances_manifest.jsonl file
    output_root : Path
        Root directory where mask files are stored (relative paths in manifest)
    min_purity : float
        Minimum purity threshold to include instance (default: 0.0, no filtering)
    min_area : int
        Minimum mask area in pixels (default: 100)
    transform : callable, optional
        Transform to apply to images
    crop_to_bbox : bool
        If True, crop image to instance bbox. If False, use full image with mask (default: True)
    excluded_classes : list of int, optional
        Class labels to exclude from training (e.g., order/genus level classes)
    """
    
    def __init__(
        self,
        manifest_path: Path,
        output_root: Path,
        min_purity: float = 0.0,
        min_area: int = 100,
        transform=None,
        crop_to_bbox: bool = True,
        excluded_classes: Optional[List[int]] = None,
    ):
        self.manifest_path = Path(manifest_path)
        self.output_root = Path(output_root)
        self.min_purity = min_purity
        self.min_area = min_area
        self.transform = transform
        self.crop_to_bbox = crop_to_bbox
        self.excluded_classes = set(excluded_classes) if excluded_classes else set()
        
        # Load instances from manifest
        self.instances = []
        excluded_count = 0
        with self.manifest_path.open('r') as f:
            for line in f:
                if line.strip():
                    instance = json.loads(line)
                    # Filter by purity, area, and excluded classes
                    if instance['label'] in self.excluded_classes:
                        excluded_count += 1
                        continue
                    if instance['purity'] >= min_purity and instance['area'] >= min_area:
                        self.instances.append(instance)
        
        if len(self.instances) == 0:
            raise ValueError(f"No valid instances found in {manifest_path}")
        
        print(f"Loaded {len(self.instances)} instances from {manifest_path}")
        print(f"  Min purity: {min_purity:.2f}, Min area: {min_area}")
        if excluded_count > 0:
            print(f"  Excluded {excluded_count} instances from classes: {sorted(self.excluded_classes)}")
        
        # Compute label distribution
        label_counts = {}
        for inst in self.instances:
            label = inst['label']
            label_counts[label] = label_counts.get(label, 0) + 1
        
        print(f"  Label distribution: {len(label_counts)} classes")
        for label in sorted(label_counts.keys())[:10]:  # Show first 10
            print(f"    Class {label}: {label_counts[label]} instances")
    
    def __len__(self) -> int:
        return len(self.instances)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        instance = self.instances[idx]
        
        # Load image
        image_path = Path(instance['source_image_path'])
        image = Image.open(image_path).convert('RGB')
        image_np = np.array(image)
        
        # Load mask
        # The mask path in the manifest is relative to the manifest file's directory
        manifest_dir = self.manifest_path.parent
        mask_path = manifest_dir / instance['mask_path']
        mask = Image.open(mask_path).convert('L')
        mask_np = (np.array(mask) > 127).astype(np.float32)  # Binary mask
        
        # Get bbox and label
        bbox = instance['bbox']  # [x1, y1, x2, y2]
        label = instance['label']
        
        if self.crop_to_bbox:
            # Crop to bounding box
            x1, y1, x2, y2 = [int(x) for x in bbox]
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(image_np.shape[1], x2), min(image_np.shape[0], y2)
            
            image_crop = image_np[y1:y2, x1:x2]
            mask_crop = mask_np[y1:y2, x1:x2]
            
            # Convert to PIL for transforms
            image_pil = Image.fromarray(image_crop)
            mask_pil = Image.fromarray((mask_crop * 255).astype(np.uint8))
        else:
            # Use full image
            image_pil = Image.fromarray(image_np)
            mask_pil = Image.fromarray((mask_np * 255).astype(np.uint8))
        
        # Apply transforms if provided
        if self.transform:
            try:
                image_tensor, mask_tensor = self.transform(image_pil, mask_pil)
            except TypeError:
                image_tensor = self.transform(image_pil)
                mask_resized = mask_pil.resize(
                    (image_tensor.shape[2], image_tensor.shape[1]),
                    resample=Image.NEAREST,
                )
                mask_tensor = TF.to_tensor(mask_resized)
        else:
            image_tensor = TF.to_tensor(image_pil)
            mask_tensor = TF.to_tensor(mask_pil)
        
        return {
            'image': image_tensor,
            'mask': mask_tensor,
            'label': label - 1,  # Convert to 0-indexed (assuming labels start at 1)
            'purity': instance['purity'],
            'area': instance['area'],
            'image_name': instance['image'],
        }


def create_instance_dataloaders(
    train_manifest: Path,
    val_manifest: Path,
    output_root: Path,
    batch_size: int = 32,
    num_workers: int = 4,
    min_purity: float = 0.7,
    min_area: int = 100,
    image_size: Tuple[int, int] = (224, 224),
    excluded_classes: Optional[List[int]] = None,
) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:
    """
    Create train and validation dataloaders for instance classification.
    
    Parameters
    ----------
    train_manifest : Path
        Path to training manifest
    val_manifest : Path
        Path to validation manifest
    output_root : Path
        Root directory for instance data
    batch_size : int
        Batch size
    num_workers : int
        Number of data loading workers
    min_purity : float
        Minimum purity threshold
    min_area : int
        Minimum area threshold
    image_size : Tuple[int, int]
        Target image size (H, W)
    excluded_classes : list of int, optional
        Class labels to exclude from training (e.g., order/genus level classes)
    
    Returns
    -------
    train_loader : DataLoader
    val_loader : DataLoader
    """
    image_size = tuple(image_size)
    train_transform = InstanceTransform(image_size=image_size, augment=True)
    val_transform = InstanceTransform(image_size=image_size, augment=False)
    
    train_dataset = TreeInstanceDataset(
        train_manifest,
        output_root,
        min_purity=min_purity,
        min_area=min_area,
        transform=train_transform,
        crop_to_bbox=True,
        excluded_classes=excluded_classes,
    )
    
    val_dataset = TreeInstanceDataset(
        val_manifest,
        output_root,
        min_purity=min_purity,
        min_area=min_area,
        transform=val_transform,
        crop_to_bbox=True,
        excluded_classes=excluded_classes,
    )
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
    )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
    )
    
    return train_loader, val_loader
